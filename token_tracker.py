import re
import json
import datetime
import threading
import time
import os
from collections import defaultdict, deque
import tkinter as tk
from tkinter import ttk, messagebox

class TokenTracker:
    def __init__(self, main_monitor):
        self.main_monitor = main_monitor
        self.token_usage = defaultdict(list)
        self.session_tokens = {}
        self.daily_limits = {}
        
        # Token patterns - Claude'un token kullanÄ±mÄ± hakkÄ±nda verdiÄŸi bilgileri yakalamak iÃ§in
        self.token_patterns = [
            r'(?i)(\d+)\s*tokens?\s*used',
            r'(?i)tokens?\s*used:?\s*(\d+)',
            r'(?i)(\d+)\s*tokens?\s*remaining',
            r'(?i)tokens?\s*remaining:?\s*(\d+)',
            r'(?i)(\d+)\s*tokens?\s*consumed',
            r'(?i)token\s*usage:?\s*(\d+)',
            r'(?i)(\d+)\s*tokens?\s*processed',
            r'(?i)total\s*tokens?:?\s*(\d+)',
        ]
        
        # Mesaj uzunluÄŸu patterns
        self.message_patterns = [
            r'(?i)(\d+)\s*characters?',
            r'(?i)message\s*length:?\s*(\d+)',
            r'(?i)(\d+)\s*words?',
        ]
        
        # Limit patterns
        self.limit_patterns = [
            r'(?i)daily\s*limit:?\s*(\d+)',
            r'(?i)monthly\s*limit:?\s*(\d+)',
            r'(?i)usage\s*limit:?\s*(\d+)',
            r'(?i)(\d+)\s*requests?\s*per\s*day',
        ]
        
        self.token_estimates = deque(maxlen=1000)  # Son 1000 mesaj iÃ§in token tahmini
        self.create_token_ui()
        
    def create_token_ui(self):
        """Token tracking UI'si"""
        self.token_window = None
        
    def show_token_dashboard(self):
        """Token dashboard'unu gÃ¶ster"""
        if self.token_window and self.token_window.winfo_exists():
            self.token_window.lift()
            return
        
        self.token_window = tk.Toplevel()
        self.token_window.title("Token & Usage Tracker")
        self.token_window.geometry("900x700")
        
        # Ana notebook
        notebook = ttk.Notebook(self.token_window)
        notebook.pack(fill="both", expand=True, padx=10, pady=10)
        
        # Session Token Usage
        session_frame = ttk.Frame(notebook)
        notebook.add(session_frame, text="Session Token KullanÄ±mÄ±")
        self.create_session_token_ui(session_frame)
        
        # Daily Limits
        daily_frame = ttk.Frame(notebook)
        notebook.add(daily_frame, text="GÃ¼nlÃ¼k Limitler")
        self.create_daily_limits_ui(daily_frame)
        
        # Token Estimator
        estimator_frame = ttk.Frame(notebook)
        notebook.add(estimator_frame, text="Token Tahmini")
        self.create_token_estimator_ui(estimator_frame)
        
        # Usage Analytics
        analytics_frame = ttk.Frame(notebook)
        notebook.add(analytics_frame, text="KullanÄ±m Analizi")
        self.create_usage_analytics_ui(analytics_frame)
        
        self.update_token_dashboard()
    
    def create_session_token_ui(self, parent):
        """Session token kullanÄ±mÄ± UI'si"""
        # Session token listesi
        self.session_token_tree = ttk.Treeview(parent, 
                                              columns=("tokens_used", "messages", "avg_tokens", "status"), 
                                              show="tree headings")
        self.session_token_tree.heading("#0", text="Session ID")
        self.session_token_tree.heading("tokens_used", text="KullanÄ±lan Token")
        self.session_token_tree.heading("messages", text="Mesaj SayÄ±sÄ±")
        self.session_token_tree.heading("avg_tokens", text="Ort. Token/Mesaj")
        self.session_token_tree.heading("status", text="Durum")
        
        self.session_token_tree.pack(fill="both", expand=True, pady=(0, 10))
        
        # Token istatistikleri
        stats_frame = ttk.LabelFrame(parent, text="Token Ä°statistikleri", padding="10")
        stats_frame.pack(fill="x")
        
        self.token_stats_text = tk.Text(stats_frame, height=8, wrap=tk.WORD)
        self.token_stats_text.pack(fill="x")
    
    def create_daily_limits_ui(self, parent):
        """GÃ¼nlÃ¼k limitler UI'si"""
        # GÃ¼nlÃ¼k kullanÄ±m Ã¶zeti
        summary_frame = ttk.LabelFrame(parent, text="GÃ¼nlÃ¼k Ã–zet", padding="10")
        summary_frame.pack(fill="x", pady=(0, 10))
        
        self.daily_summary_text = tk.Text(summary_frame, height=6, wrap=tk.WORD)
        self.daily_summary_text.pack(fill="x")
        
        # Limit uyarÄ±larÄ±
        limits_frame = ttk.LabelFrame(parent, text="Limit UyarÄ±larÄ±", padding="5")
        limits_frame.pack(fill="both", expand=True)
        
        self.limits_tree = ttk.Treeview(limits_frame, 
                                       columns=("date", "type", "current", "limit", "percentage"), 
                                       show="headings")
        self.limits_tree.heading("date", text="Tarih")
        self.limits_tree.heading("type", text="Tip")
        self.limits_tree.heading("current", text="Mevcut")
        self.limits_tree.heading("limit", text="Limit")
        self.limits_tree.heading("percentage", text="%")
        
        self.limits_tree.pack(fill="both", expand=True)
    
    def create_token_estimator_ui(self, parent):
        """Token tahmini UI'si"""
        # Metin giriÅŸi
        input_frame = ttk.LabelFrame(parent, text="Token Tahmini", padding="10")
        input_frame.pack(fill="x", pady=(0, 10))
        
        ttk.Label(input_frame, text="Metni yapÄ±ÅŸtÄ±rÄ±n:").pack(anchor="w")
        
        self.estimate_text = tk.Text(input_frame, height=6, wrap=tk.WORD)
        self.estimate_text.pack(fill="x", pady=(5, 10))
        
        estimate_btn_frame = ttk.Frame(input_frame)
        estimate_btn_frame.pack(fill="x")
        
        ttk.Button(estimate_btn_frame, text="Token Tahmin Et", 
                  command=self.estimate_tokens).pack(side="left", padx=(0, 10))
        ttk.Button(estimate_btn_frame, text="Temizle", 
                  command=lambda: self.estimate_text.delete(1.0, tk.END)).pack(side="left")
        
        # SonuÃ§lar
        results_frame = ttk.LabelFrame(parent, text="Tahmin SonuÃ§larÄ±", padding="10")
        results_frame.pack(fill="both", expand=True)
        
        self.estimate_results = tk.Text(results_frame, wrap=tk.WORD)
        self.estimate_results.pack(fill="both", expand=True)
    
    def create_usage_analytics_ui(self, parent):
        """KullanÄ±m analizi UI'si"""
        # Analiz kontrolleri
        control_frame = ttk.Frame(parent)
        control_frame.pack(fill="x", pady=(0, 10))
        
        ttk.Label(control_frame, text="Analiz Periyodu:").pack(side="left")
        
        self.analysis_period = tk.StringVar(value="bugÃ¼n")
        period_combo = ttk.Combobox(control_frame, textvariable=self.analysis_period,
                                   values=["bugÃ¼n", "bu hafta", "bu ay", "tÃ¼mÃ¼"])
        period_combo.pack(side="left", padx=(5, 10))
        
        ttk.Button(control_frame, text="Analiz Et", 
                  command=self.analyze_usage).pack(side="left")
        
        # Analiz sonuÃ§larÄ±
        self.analytics_text = tk.Text(parent, wrap=tk.WORD)
        analytics_scrollbar = ttk.Scrollbar(parent, orient="vertical", 
                                           command=self.analytics_text.yview)
        self.analytics_text.configure(yscrollcommand=analytics_scrollbar.set)
        
        self.analytics_text.pack(side="left", fill="both", expand=True)
        analytics_scrollbar.pack(side="right", fill="y")
    
    def track_token_usage(self, session_id, text, message_type):
        """Token kullanÄ±mÄ±nÄ± takip et"""
        # Explicit token bilgisi varsa yakala
        explicit_tokens = self.extract_explicit_tokens(text)
        
        # Token tahmini yap
        estimated_tokens = self.estimate_tokens_from_text(text)
        
        timestamp = datetime.datetime.now()
        
        token_entry = {
            'timestamp': timestamp.isoformat(),
            'session_id': session_id,
            'message_type': message_type,
            'text_length': len(text),
            'word_count': len(text.split()),
            'estimated_tokens': estimated_tokens,
            'explicit_tokens': explicit_tokens,
            'text_preview': text[:100] + "..." if len(text) > 100 else text
        }
        
        # Session token'larÄ±nÄ± gÃ¼ncelle
        if session_id not in self.session_tokens:
            self.session_tokens[session_id] = {
                'total_estimated': 0,
                'total_explicit': 0,
                'message_count': 0,
                'start_time': timestamp,
                'last_activity': timestamp,
                'entries': []
            }
        
        session_data = self.session_tokens[session_id]
        session_data['total_estimated'] += estimated_tokens
        if explicit_tokens:
            session_data['total_explicit'] += explicit_tokens
        session_data['message_count'] += 1
        session_data['last_activity'] = timestamp
        session_data['entries'].append(token_entry)
        
        # GÃ¼nlÃ¼k kullanÄ±m gÃ¼ncelle
        today = timestamp.strftime('%Y-%m-%d')
        if today not in self.daily_limits:
            self.daily_limits[today] = {
                'estimated_tokens': 0,
                'explicit_tokens': 0,
                'message_count': 0,
                'sessions': set()
            }
        
        daily_data = self.daily_limits[today]
        daily_data['estimated_tokens'] += estimated_tokens
        if explicit_tokens:
            daily_data['explicit_tokens'] += explicit_tokens
        daily_data['message_count'] += 1
        daily_data['sessions'].add(session_id)
        
        # Token estimate buffer'a ekle
        self.token_estimates.append(token_entry)
        
        # Dosyaya kaydet
        self.save_token_data(token_entry)
        
        return token_entry
    
    def extract_explicit_tokens(self, text):
        """Claude'un verdiÄŸi explicit token bilgilerini Ã§Ä±kar"""
        for pattern in self.token_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            if matches:
                try:
                    return int(matches[0])
                except (ValueError, IndexError):
                    continue
        return None
    
    def estimate_tokens_from_text(self, text):
        """Metinden token tahmini yap"""
        # Basit tahmin: 
        # - Ortalama 4 karakter = 1 token
        # - Ã–zel karakterler ve boÅŸluklar dikkate alÄ±nÄ±r
        
        # Kelime sayÄ±sÄ± bazlÄ± tahmin
        words = text.split()
        word_based_estimate = len(words) * 1.3  # Ortalama 1.3 token per word
        
        # Karakter sayÄ±sÄ± bazlÄ± tahmin
        char_based_estimate = len(text) / 4  # 4 char per token
        
        # Ä°ki tahminin ortalamasÄ±
        estimated_tokens = int((word_based_estimate + char_based_estimate) / 2)
        
        return max(1, estimated_tokens)  # En az 1 token
    
    def estimate_tokens(self):
        """UI'den token tahmini yap"""
        text = self.estimate_text.get(1.0, tk.END).strip()
        if not text:
            return
        
        estimated = self.estimate_tokens_from_text(text)
        char_count = len(text)
        word_count = len(text.split())
        
        results = f"""ğŸ“Š TOKEN TAHMÄ°NÄ° SONUÃ‡LARI
{'='*40}

ğŸ“ METÄ°N Ä°STATÄ°STÄ°KLERÄ°:
â€¢ Karakter SayÄ±sÄ±: {char_count:,}
â€¢ Kelime SayÄ±sÄ±: {word_count:,}
â€¢ SatÄ±r SayÄ±sÄ±: {text.count(chr(10)) + 1}

ğŸ¯ TOKEN TAHMÄ°NLERÄ°:
â€¢ Tahmini Token: {estimated:,}
â€¢ Kelime BazlÄ±: {int(word_count * 1.3):,}
â€¢ Karakter BazlÄ±: {int(char_count / 4):,}

ğŸ’° MALÄ°YET TAHMÄ°NÄ° (yaklaÅŸÄ±k):
â€¢ GPT-4 Input: ${estimated * 0.00003:.4f}
â€¢ GPT-4 Output: ${estimated * 0.00006:.4f}

âš ï¸  NOT: Bu tahminler yaklaÅŸÄ±ktÄ±r. GerÃ§ek token 
kullanÄ±mÄ± model ve iÃ§eriÄŸe gÃ¶re deÄŸiÅŸebilir."""
        
        self.estimate_results.delete(1.0, tk.END)
        self.estimate_results.insert(1.0, results)
    
    def analyze_usage(self):
        """KullanÄ±m analizi yap"""
        period = self.analysis_period.get()
        current_time = datetime.datetime.now()
        
        # Periyot hesapla
        if period == "bugÃ¼n":
            start_date = current_time.replace(hour=0, minute=0, second=0, microsecond=0)
        elif period == "bu hafta":
            days_since_monday = current_time.weekday()
            start_date = current_time - datetime.timedelta(days=days_since_monday)
            start_date = start_date.replace(hour=0, minute=0, second=0, microsecond=0)
        elif period == "bu ay":
            start_date = current_time.replace(day=1, hour=0, minute=0, second=0, microsecond=0)
        else:  # tÃ¼mÃ¼
            start_date = datetime.datetime.min
        
        # Veri filtrele
        relevant_entries = []
        for session_data in self.session_tokens.values():
            for entry in session_data['entries']:
                entry_time = datetime.datetime.fromisoformat(entry['timestamp'])
                if entry_time >= start_date:
                    relevant_entries.append(entry)
        
        if not relevant_entries:
            self.analytics_text.delete(1.0, tk.END)
            self.analytics_text.insert(1.0, "SeÃ§ilen periyot iÃ§in veri bulunamadÄ±.")
            return
        
        # Analiz yap
        total_estimated = sum(e['estimated_tokens'] for e in relevant_entries)
        total_explicit = sum(e['explicit_tokens'] for e in relevant_entries if e['explicit_tokens'])
        total_messages = len(relevant_entries)
        unique_sessions = len(set(e['session_id'] for e in relevant_entries))
        
        avg_tokens_per_message = total_estimated / total_messages if total_messages > 0 else 0
        
        # Mesaj tipi analizi
        message_types = defaultdict(int)
        for entry in relevant_entries:
            message_types[entry['message_type']] += 1
        
        # En aktif session'lar
        session_usage = defaultdict(int)
        for entry in relevant_entries:
            session_usage[entry['session_id']] += entry['estimated_tokens']
        
        top_sessions = sorted(session_usage.items(), key=lambda x: x[1], reverse=True)[:5]
        
        analysis_result = f"""ğŸ“ˆ KULLANIM ANALÄ°ZÄ° - {period.upper()}
{'='*50}

ğŸ“Š GENEL Ä°STATÄ°STÄ°KLER:
â€¢ Toplam Mesaj: {total_messages:,}
â€¢ Unique Session: {unique_sessions}
â€¢ Tahmini Token: {total_estimated:,}
â€¢ Explicit Token: {total_explicit:,} ({total_explicit/total_estimated*100 if total_estimated > 0 else 0:.1f}%)
â€¢ Ort. Token/Mesaj: {avg_tokens_per_message:.1f}

ğŸ“ MESAJ TÄ°PÄ° DAÄILIMI:
"""
        
        for msg_type, count in message_types.items():
            percentage = count / total_messages * 100 if total_messages > 0 else 0
            analysis_result += f"â€¢ {msg_type}: {count:,} (%{percentage:.1f})\n"
        
        analysis_result += f"""
ğŸ† EN AKTÄ°F SESSION'LAR:
"""
        
        for i, (session_id, tokens) in enumerate(top_sessions, 1):
            analysis_result += f"{i}. {session_id}: {tokens:,} token\n"
        
        # GÃ¼nlÃ¼k trend
        daily_usage = defaultdict(int)
        for entry in relevant_entries:
            date = datetime.datetime.fromisoformat(entry['timestamp']).strftime('%Y-%m-%d')
            daily_usage[date] += entry['estimated_tokens']
        
        if len(daily_usage) > 1:
            analysis_result += f"""
ğŸ“… GÃœNLÄ°K TREND:
"""
            for date in sorted(daily_usage.keys())[-7:]:  # Son 7 gÃ¼n
                analysis_result += f"â€¢ {date}: {daily_usage[date]:,} token\n"
        
        self.analytics_text.delete(1.0, tk.END)
        self.analytics_text.insert(1.0, analysis_result)
    
    def check_daily_limits(self):
        """GÃ¼nlÃ¼k limitleri kontrol et"""
        today = datetime.datetime.now().strftime('%Y-%m-%d')
        
        if today not in self.daily_limits:
            return
        
        daily_data = self.daily_limits[today]
        estimated_tokens = daily_data['estimated_tokens']
        
        # VarsayÄ±lan gÃ¼nlÃ¼k limitler (tahmin)
        WARNING_THRESHOLD = 50000  # 50K token
        CRITICAL_THRESHOLD = 80000  # 80K token
        
        if estimated_tokens > CRITICAL_THRESHOLD:
            self.main_monitor.add_alert('token_critical', 
                                      f"CRITICAL: GÃ¼nlÃ¼k token kullanÄ±mÄ± {estimated_tokens:,} (Limit: {CRITICAL_THRESHOLD:,})")
        elif estimated_tokens > WARNING_THRESHOLD:
            self.main_monitor.add_alert('token_warning', 
                                      f"WARNING: GÃ¼nlÃ¼k token kullanÄ±mÄ± {estimated_tokens:,} (UyarÄ±: {WARNING_THRESHOLD:,})")
    
    def save_token_data(self, token_entry):
        """Token verisini dosyaya kaydet"""
        date = datetime.datetime.now().strftime('%Y%m%d')
        token_file = f"claude_session_data/tokens/token_usage_{date}.json"
        
        os.makedirs(os.path.dirname(token_file), exist_ok=True)
        
        with open(token_file, 'a', encoding='utf-8') as f:
            json.dump(token_entry, f, ensure_ascii=False)
            f.write('\n')
    
    def load_token_data(self):
        """Token verilerini yÃ¼kle"""
        token_dir = "claude_session_data/tokens"
        if not os.path.exists(token_dir):
            return
        
        for filename in os.listdir(token_dir):
            if filename.startswith('token_usage_') and filename.endswith('.json'):
                file_path = os.path.join(token_dir, filename)
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        for line in f:
                            if line.strip():
                                entry = json.loads(line)
                                # Restore session data
                                session_id = entry['session_id']
                                if session_id not in self.session_tokens:
                                    self.session_tokens[session_id] = {
                                        'total_estimated': 0,
                                        'total_explicit': 0,
                                        'message_count': 0,
                                        'entries': []
                                    }
                                
                                self.session_tokens[session_id]['entries'].append(entry)
                                self.token_estimates.append(entry)
                except Exception as e:
                    print(f"Token data load error: {e}")
    
    def update_token_dashboard(self):
        """Token dashboard'unu gÃ¼ncelle"""
        if not self.token_window or not self.token_window.winfo_exists():
            return
        
        # Session token tree gÃ¼ncelle
        for item in self.session_token_tree.get_children():
            self.session_token_tree.delete(item)
        
        for session_id, session_data in self.session_tokens.items():
            total_tokens = session_data['total_estimated']
            message_count = session_data['message_count']
            avg_tokens = total_tokens / message_count if message_count > 0 else 0
            
            # Session durumu
            status = "Aktif" if session_id in [s for s in self.main_monitor.sessions.keys() 
                                             if self.main_monitor.sessions[s]['status'] == 'active'] else "KapalÄ±"
            
            self.session_token_tree.insert('', 'end',
                                          text=session_id,
                                          values=(f"{total_tokens:,}",
                                                 message_count,
                                                 f"{avg_tokens:.1f}",
                                                 status))
        
        # Token istatistikleri gÃ¼ncelle
        total_estimated = sum(data['total_estimated'] for data in self.session_tokens.values())
        total_messages = sum(data['message_count'] for data in self.session_tokens.values())
        total_sessions = len(self.session_tokens)
        
        today = datetime.datetime.now().strftime('%Y-%m-%d')
        today_data = self.daily_limits.get(today, {'estimated_tokens': 0, 'message_count': 0})
        
        stats_text = f"""ğŸ¯ TOKEN Ä°STATÄ°STÄ°KLERÄ°
{'='*30}

ğŸ“Š TOPLAM:
â€¢ Sessions: {total_sessions}
â€¢ Mesajlar: {total_messages:,}
â€¢ Tahmini Token: {total_estimated:,}

ğŸ“… BUGÃœN:
â€¢ Mesajlar: {today_data['message_count']:,}
â€¢ Tahmini Token: {today_data['estimated_tokens']:,}

ğŸ“ˆ ORTALAMALAR:
â€¢ Token/Session: {total_estimated/total_sessions if total_sessions > 0 else 0:.1f}
â€¢ Token/Mesaj: {total_estimated/total_messages if total_messages > 0 else 0:.1f}

ğŸ• Son GÃ¼ncelleme: {datetime.datetime.now().strftime('%H:%M:%S')}"""
        
        self.token_stats_text.delete(1.0, tk.END)
        self.token_stats_text.insert(1.0, stats_text)
        
        # GÃ¼nlÃ¼k limitler kontrol et
        self.check_daily_limits()
        
        # 10 saniye sonra tekrar gÃ¼ncelle
        self.token_window.after(10000, self.update_token_dashboard)
    
    def start_monitoring(self):
        """Token monitoring'i baÅŸlat"""
        # Load existing data
        self.load_token_data()
        
        # Her dakika limit kontrolÃ¼ yap
        def limit_check_loop():
            while True:
                self.check_daily_limits()
                time.sleep(60)  # 1 dakika
        
        limit_thread = threading.Thread(target=limit_check_loop, daemon=True)
        limit_thread.start()

if __name__ == "__main__":
    # Test iÃ§in
    class MockMonitor:
        def __init__(self):
            self.sessions = {}
        
        def add_alert(self, alert_type, message, session_id=None):
            print(f"Alert: {alert_type} - {message}")
    
    mock_monitor = MockMonitor()
    tracker = TokenTracker(mock_monitor)
    
    # Test data
    test_text = "Bu bir test mesajÄ±dÄ±r. Token tahmini iÃ§in kullanÄ±lacaktÄ±r. Claude'un verdiÄŸi yanÄ±tlarÄ± takip etmek iÃ§in kullanÄ±yoruz."
    tracker.track_token_usage("test_session", test_text, "user_prompt")
    
    tracker.show_token_dashboard()
    tracker.start_monitoring()
    
    # Ana loop
    root = tk.Tk()
    root.withdraw()
    root.mainloop()